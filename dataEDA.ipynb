{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/03 18:24:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# initialize spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"News Data Analysis\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xlog:disable\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xlog:disable\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", IntegerType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"archive\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"probability\", FloatType(), True),\n",
    "    StructField(\"keywords\", MapType(StringType(), IntegerType()), True),\n",
    "    StructField(\"sentiment\", FloatType(), True),\n",
    "    #StructField(\"status\", StringType(), True),\n",
    "    #StructField(\"error\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**global data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23430Kb max_used=23646Kb free=107641Kb\n",
      " bounds [0x000000010898c000, 0x000000010a0dc000, 0x000000011098c000]\n",
      " total_blobs=9745 nmethods=8824 adapters=836\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links taken into consideration:  2323500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actually news:  546241\n",
      "which are distributed across 3521 files.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").schema(schema).load(\"data/news/status=success\")\n",
    "df02 = spark.read.format(\"json\").schema(schema).load(\"data/news/status=error\")\n",
    "df03 = spark.read.format(\"json\").schema(schema).load(\"data/news/status=duplicate\")\n",
    "df04 = spark.read.format(\"json\").schema(schema).load(\"data/news/status=notnews\")\n",
    "\n",
    "print(\"Total number of links taken into consideration: \", df.count() + df02.count() + df03.count() + df04.count())\n",
    "print(\"Number of actually news: \", df.count())\n",
    "print(f\"which are distributed across {len(df.inputFiles())} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filtering query**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmarking the performance of the query\n",
    "\n",
    "with cache\n",
    "\n",
    "```\n",
    "Galp: 18.44\n",
    "galp: 13.70\n",
    "Millenium: 16.94\n",
    "```\n",
    "\n",
    "without cache\n",
    "\n",
    "```\n",
    "Millenium: > 31\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Galp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "query_col_counts = F.col(\"keywords\").getItem(query)\n",
    "df_with_query = df.filter(query_col_counts.isNotNull() & (query_col_counts > 4)).cache()\n",
    "\n",
    "print(f\"Number of news with the query: {df_with_query.count()}\")\n",
    "\n",
    "df_with_query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "result = (\n",
    "    df_with_query.rdd\n",
    "    .flatMap(lambda row: [\n",
    "        (key, (value,\n",
    "               {row[\"timestamp\"]: value},\n",
    "               row[\"sentiment\"]*value,\n",
    "               {row[\"source\"]: 1},\n",
    "               [row[\"archive\"]])) for key, value in row[\"keywords\"].items()\n",
    "    ])\n",
    "    .reduceByKey(lambda a, b: (\n",
    "        a[0] + b[0],  # Sum count values\n",
    "        {ts: a[1].get(ts, 0) + b[1].get(ts, 0) for ts in set(a[1]) | set(b[1])},  # Merge timestamp counts\n",
    "        a[2] + b[2],  # Sum sentiment values\n",
    "        {source: a[3].get(source, 0) + b[3].get(source, 0) for source in set(a[3]) | set(b[3])},  # Merge source counts\n",
    "        a[4] + b[4]  # Merge archive lists\n",
    "    ))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Convert to dictionary\n",
    "output = {key: {\"count\": value[0],\n",
    "                \"date\": value[1],\n",
    "                \"sentiment\": value[2]/value[0],\n",
    "                \"source\": value[3],\n",
    "                \"news\": value[4]} for key, value in result}\n",
    "\n",
    "print(f\"The outputs is a {type(output)} with {len(output)} topics related to {query}.\")\n",
    "print(f\"The keys are the topics and the values are dictionaries with the following keys: {list(output.values())[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'data/keywords_test.json'\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "#with open(file_path, 'w') as json_file:\n",
    "#    json.dump(output, json_file, indent=4)\n",
    "\n",
    "#print(f\"JSON data saved to {file_path}\")\n",
    "\n",
    "# Load the JSON data back into a Python dictionary\n",
    "#with open(file_path, 'r') as json_file:\n",
    "#    loaded_data = json.load(json_file)\n",
    "\n",
    "# Print loaded data\n",
    "#print(\"Loaded data:\")\n",
    "#loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**creating the graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just use the graph.py file to create it\n",
    "\n",
    "the variable output is the input for the graph.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**query info and statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of news (add pie plot showing % ?) [done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of news with the query: {df_with_query.count()} out of {df.count()}, between {1900} and {2025}.\")\n",
    "print(\"Note: may replace in the future the df.count() with a fixed integer, avoiding computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- source of news [done and implemented]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Group by the column and count the values\n",
    "value_counts_df = df_with_query.groupBy('source').count().toPandas()\n",
    "\n",
    "# Extract labels and values directly\n",
    "labels = value_counts_df['source']\n",
    "values = value_counts_df['count']\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=labels,\n",
    "    values=values,\n",
    "    hoverinfo='label+value+percent',\n",
    "    hovertemplate=\"<b>%{label}</b><br>Not√≠cias: %{value}<br>Percentagem: %{percent:.2%}<extra></extra>\"\n",
    ")])\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='inside',\n",
    "    textinfo='label',\n",
    "    textfont_size=12\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    margin=dict(t=25, b=25, l=0, r=0)\n",
    ")\n",
    "\n",
    "fig.show(\n",
    "    config={'displayModeBar': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of news and top10 words by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import collect_list\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "traducao_meses = {\n",
    "    \"January\": \"Janeiro\", \"February\": \"Fevereiro\", \"March\": \"Mar√ßo\",\n",
    "    \"April\": \"Abril\", \"May\": \"Maio\", \"June\": \"Junho\",\n",
    "    \"July\": \"Julho\", \"August\": \"Agosto\", \"September\": \"Setembro\",\n",
    "    \"October\": \"Outubro\", \"November\": \"Novembro\", \"December\": \"Dezembro\"\n",
    "}\n",
    "\n",
    "news_by_month = (\n",
    "    df_with_query\n",
    "    .groupBy('timestamp')\n",
    "    .agg(F.count('archive').alias('count_of_news'))\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "keywords_by_month = (\n",
    "    df_with_query\n",
    "    .select('*', F.explode('keywords'))\n",
    "    .groupBy(\"timestamp\", \"key\")\n",
    "    .agg(F.sum(\"value\").alias(\"key_mentions\"))\n",
    "    .filter(F.col(\"key\") != query)\n",
    "    .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"timestamp\").orderBy(F.desc(\"key_mentions\"))))\n",
    "    .filter(F.col(\"rank\") <= 5)\n",
    "    .groupBy(\"timestamp\")\n",
    "    .agg(collect_list(\"key\").alias(\"top5_keywords\"))\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "news_history = news_by_month.merge(keywords_by_month, on=\"timestamp\", how=\"inner\")\n",
    "news_history[\"timestamp\"] = pd.to_datetime(news_history[\"timestamp\"].astype(str), format='%Y%m')\n",
    "\n",
    "min_date = news_history[\"timestamp\"].min()\n",
    "max_date = news_history[\"timestamp\"].max()\n",
    "full_range = pd.date_range(start=min_date, end=max_date, freq='MS')\n",
    "\n",
    "news_history = news_history.set_index(\"timestamp\").reindex(full_range).fillna(0).reset_index()\n",
    "news_history = news_history.rename(columns={\"index\": \"timestamp\"})\n",
    "news_history = news_history.sort_values(by=\"timestamp\")\n",
    "\n",
    "news_history[\"data_formatada\"] = news_history[\"timestamp\"].dt.strftime(\"%B de %Y\").replace(traducao_meses, regex=True)\n",
    "news_history[\"top5_keywords\"] = news_history[\"top5_keywords\"].apply(\n",
    "    lambda words: \"-\" if words == 0 else \"<br>\".join([f\"{i+1}. {word}\" for i, word in enumerate(words)])\n",
    ")\n",
    "\n",
    "print(news_history.columns)\n",
    "\n",
    "# Create Plotly figure\n",
    "fig = px.line(\n",
    "    news_history,\n",
    "    x=\"timestamp\",\n",
    "    y=\"count_of_news\",\n",
    "    line_shape=\"linear\",\n",
    "    custom_data=news_history[[\"data_formatada\", \"top5_keywords\"]],\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<b>Data:</b> %{customdata[0]}<br>\"\n",
    "                  \"<b>üìä Not√≠cias:</b> %{y}<br>\"\n",
    "                  \"<b>Top 5 T√≥picos:</b><br>%{customdata[1]}\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Data\",\n",
    "    yaxis_title=\"Quantidade de Not√≠cias\",\n",
    "    paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    plot_bgcolor=\"pink\",\n",
    "     margin=dict(t=0, b=0, l=0, r=0)\n",
    ")\n",
    "\n",
    "fig.update_traces(line=dict(color='rgb(255, 255, 0)'),\n",
    "                  hoverlabel=dict(bgcolor='rgb(0, 255, 0)',\n",
    "                                  font=dict(color='black')))\n",
    "\n",
    "\n",
    "fig.update_xaxes(tickformat=\"%m/%Y\")\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load word count data\n",
    "word_counts = {key: value[\"count\"] for key, value in output.items()}  # Ensure this is defined\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (1920, 500)  # Fixed background size (canvas)\n",
    "TEXT = \"ant√≥nio pinto da costa\"\n",
    "FONT_PATH = \"assets/LoveDays-2v7Oe.ttf\"\n",
    "MAX_FONT_SIZE = 500  # Max possible font size\n",
    "\n",
    "# Load font\n",
    "font = ImageFont.truetype(FONT_PATH, size=MAX_FONT_SIZE)\n",
    "\n",
    "def get_optimal_font_size(text, font_path, max_size, image_size):\n",
    "    \"\"\"Determine the best font size to fit within the image.\"\"\"\n",
    "    for size in range(max_size, 5, -5):  # Step down in increments\n",
    "        font = ImageFont.truetype(font_path, size=size)\n",
    "        temp_img = Image.new(\"RGBA\", image_size, (0, 0, 0, 0))\n",
    "        draw = ImageDraw.Draw(temp_img)\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "\n",
    "        if text_width <= image_size[0] * 0.8 and text_height <= image_size[1] * 1:  \n",
    "            return font, bbox  # Fit within 80% width & 50% height\n",
    "\n",
    "    return font, bbox  # Return the smallest size if no fit\n",
    "\n",
    "# Get optimal font size\n",
    "font, bbox = get_optimal_font_size(TEXT, FONT_PATH, MAX_FONT_SIZE, IMAGE_SIZE)\n",
    "\n",
    "# Calculate centered position\n",
    "text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "x_offset = (IMAGE_SIZE[0] - text_width) // 2\n",
    "y_offset = (IMAGE_SIZE[1] - text_height) // 2\n",
    "\n",
    "# Create fixed-size image (1920x1080)\n",
    "final_img = Image.new(\"RGBA\", IMAGE_SIZE, (0, 0, 0, 0))\n",
    "draw = ImageDraw.Draw(final_img)\n",
    "\n",
    "# Draw centered text\n",
    "draw.text((x_offset, y_offset), TEXT, fill=(255, 105, 180), font=font)  # Pink text\n",
    "\n",
    "# Generate mask for WordCloud (ensure proper alpha channel handling)\n",
    "mask = np.array(final_img.convert(\"L\"))  # Convert to grayscale to use as a mask\n",
    "\n",
    "# Generate WordCloud\n",
    "wc = WordCloud(\n",
    "    width=IMAGE_SIZE[0], height=IMAGE_SIZE[1],\n",
    "    background_color=None, min_font_size=5, mode=\"RGBA\",\n",
    "    colormap=\"winter\", mask=~mask, contour_color=\"black\"\n",
    ").generate_from_frequencies(word_counts)\n",
    "\n",
    "# Convert WordCloud to image\n",
    "wc_image = wc.to_image()\n",
    "\n",
    "# Merge word cloud and text\n",
    "final_img = Image.alpha_composite(final_img, wc_image)\n",
    "\n",
    "# Show final image\n",
    "plt.figure(figsize=(IMAGE_SIZE[0] / 100, IMAGE_SIZE[1] / 100))  # Adjust figure size\n",
    "plt.imshow(final_img)\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- search for a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_topic = \"EDP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of news where both {query} and {search_topic} are mentioned: {output[search_topic]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "query = \"Galp\"\n",
    "search_topic = \"EDP\"\n",
    "\n",
    "#traducao_meses = {\n",
    "#    \"January\": \"Janeiro\", \"February\": \"Fevereiro\", \"March\": \"Mar√ßo\",\n",
    "#    \"April\": \"Abril\", \"May\": \"Maio\", \"June\": \"Junho\",\n",
    "#    \"July\": \"Julho\", \"August\": \"Agosto\", \"September\": \"Setembro\",\n",
    "#    \"October\": \"Outubro\", \"November\": \"Novembro\", \"December\": \"Dezembro\"\n",
    "#}\n",
    "\n",
    "# number of news per month\n",
    "news_by_month = (\n",
    "    df_with_query\n",
    "    .groupBy('timestamp')\n",
    "    .agg(F.count('archive').alias('count_of_news'))\n",
    "    .toPandas()\n",
    ")\n",
    "news_by_month[\"timestamp\"] = pd.to_datetime(news_by_month[\"timestamp\"].astype(str), format='%Y%m')\n",
    "\n",
    "# number of mentions of the specific keyword\n",
    "specific_keyword = pd.DataFrame(list(output[search_topic][\"date\"].items()), columns=[\"date\", \"count_specific_keyword\"])\n",
    "specific_keyword[\"date\"] = pd.to_datetime(specific_keyword[\"date\"], format=\"%Y%m\")\n",
    "\n",
    "# merge the two dataframes\n",
    "news_history = news_by_month.merge(specific_keyword, left_on=\"timestamp\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "# create full data range\n",
    "min_date = news_history[\"timestamp\"].min()\n",
    "max_date = news_history[\"timestamp\"].max()\n",
    "full_range = pd.date_range(start=min_date, end=max_date, freq='MS')\n",
    "news_history = news_history.set_index(\"timestamp\").reindex(full_range).fillna(0).reset_index()\n",
    "news_history = news_history.rename(columns={\"index\": \"timestamp\"})\n",
    "news_history = news_history.sort_values(by=\"timestamp\")\n",
    "\n",
    "#news_history[\"data_formatada\"] = news_history[\"timestamp\"].dt.strftime(\"%B de %Y\").replace(traducao_meses, regex=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=news_history[\"timestamp\"],\n",
    "    y=news_history[\"count_of_news\"],\n",
    "    mode=\"lines\",\n",
    "    name=f\"Not√≠cias sobre {query}\",\n",
    "    hovertemplate=\"%{y}\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=news_history[\"timestamp\"],\n",
    "    y=news_history[\"count_specific_keyword\"],\n",
    "    mode=\"lines\",\n",
    "    name=f\"Men√ß√µes de {search_topic} em not√≠cias sobre {query}\",\n",
    "    hovertemplate=\"%{y}\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Data\",\n",
    "    yaxis_title=\"Contagem\",\n",
    "    hovermode=\"x unified\",\n",
    "    paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    margin=dict(t=0, b=0, l=0, r=0),\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=True,\n",
    "        zerolinecolor=\"black\",\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        range=[0, max(news_history[\"count_of_news\"].max(), news_history[\"count_specific_keyword\"].max()) * 1.1],\n",
    "        showgrid=True,  \n",
    "        gridcolor=\"lightgray\",  \n",
    "        zeroline=True,\n",
    "        zerolinecolor=\"black\",\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0.02,\n",
    "        y=0.98,\n",
    "        bgcolor=\"rgba(255,255,255,1)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.data[0].update(line=dict(color='rgba(101, 110, 242, 0.3)'))\n",
    "fig.data[1].update(line=dict(color='rgb(101, 110, 242)'))\n",
    "\n",
    "fig.update_xaxes(tickformat=\"%m/%Y\")\n",
    "\n",
    "fig.show(config={'displayModeBar': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Sentiment Indicator</title>\n",
    "    <style>\n",
    "        .bar-container {\n",
    "            position: relative;\n",
    "            width: 300px;\n",
    "            height: 20px;\n",
    "            background: linear-gradient(to right, red, grey, green);\n",
    "            border-radius: 5px;\n",
    "            margin: 20px;\n",
    "        }\n",
    "        .arrow {\n",
    "            position: absolute;\n",
    "            top: 25px;\n",
    "            left: 50%; /* Default position, will be adjusted with JS */\n",
    "            transform: translateX(-50%);\n",
    "            font-size: 20px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"bar-container\" id=\"bar\">\n",
    "        <div class=\"arrow\" id=\"arrow\">‚ñº</div>\n",
    "    </div>\n",
    "    <script>\n",
    "        function updateSentiment(sentiment) {\n",
    "            let percentage = ((sentiment + 1) / 2) * 100; // Convert range (-1 to 1) to (0% to 100%)\n",
    "            document.getElementById(\"arrow\").style.left = percentage + \"%\";\n",
    "        }\n",
    "        \n",
    "        updateSentiment({{ output[search_topic][\"sentiment\"] }}); // Example: Update sentiment position\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "output[search_topic][\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Group by the column and count the values\n",
    "sources = output[search_topic]['source']\n",
    "\n",
    "# Extract labels and values directly\n",
    "labels = list(sources.keys())\n",
    "values = list(sources.values())\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=labels,\n",
    "    values=values,\n",
    "    hoverinfo='label+value+percent',\n",
    "    hovertemplate=\"<b>%{label}</b><br>Not√≠cias: %{value}<br>Percentagem: %{percent:.2%}<extra></extra>\"\n",
    ")])\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='inside',\n",
    "    textinfo='label',\n",
    "    textfont_size=12\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    margin=dict(t=25, b=25, l=0, r=0)\n",
    ")\n",
    "\n",
    "fig.show(\n",
    "    config={'displayModeBar': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def news_topicrelation(json_searchtopic_data):\n",
    "\n",
    "    company_logos = [\n",
    "        'record.png',\n",
    "        'noticiasaominuto.png',\n",
    "        'lusa.png',\n",
    "        'jornaldenegocios.png',\n",
    "        'tsf.png',\n",
    "        'sicnoticias.png',\n",
    "        'expresso.png',\n",
    "        'publico.png',\n",
    "        'dn.png',\n",
    "        'observador.png',\n",
    "        'sapo.png',\n",
    "        'iol.png',\n",
    "        'rtp.png',\n",
    "        'cnn.png',\n",
    "        'cmjornal.png',\n",
    "        'jn.png',\n",
    "        'nit.png',\n",
    "        'dinheirovivo.png',\n",
    "        'aeiou.png',\n",
    "    ]\n",
    "\n",
    "    divs = {}\n",
    "    titles = set()\n",
    "\n",
    "    for url in json_searchtopic_data[\"news\"]:\n",
    "        link_content = url.split(\"/\")\n",
    "\n",
    "        # set title\n",
    "        title = link_content[-1] if link_content[-1] != \"\" else link_content[-2]\n",
    "        title = title.split(\"?\")[0].split(\"_\")[0]\n",
    "        title = re.sub(r'^\\d{4}-\\d{2}-\\d{2}-', '', title)\n",
    "        title = title.lower() if \"-\" in title else \"Sem t√≠tulo dispon√≠vel...\"\n",
    "\n",
    "        # check if title is repeated\n",
    "        if title not in titles:\n",
    "            titles.add(title)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # set date\n",
    "        date = link_content[5]\n",
    "        date = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "\n",
    "        # set source\n",
    "        company = link_content[8].replace(\"www.\", \"\")\n",
    "\n",
    "        # get source logo if exists\n",
    "        is_there_a_logo = False\n",
    "        for img in company_logos:\n",
    "            if img[:-4] in company:\n",
    "                img = f\"0news_logos/{img}\"\n",
    "                is_there_a_logo = True\n",
    "                break\n",
    "        if not is_there_a_logo:\n",
    "            img = \"0news_logos/404.png\"\n",
    "\n",
    "        # create the div\n",
    "        div = f\"\"\"\n",
    "                            <div class=\"testimonial-item bg-transparent border rounded text-white p-4\">\n",
    "                                <i class=\"fa fa-quote-left fa-2x mb-3\"></i>\n",
    "                                <a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"d-block text-decoration-none mb-3\" style=\"color: inherit; height: 4.5em;\">\n",
    "                                    <p style=\"margin: 0; text-overflow: ellipsis; overflow: hidden; display: -webkit-box; -webkit-line-clamp: 3; -webkit-box-orient: vertical; height: 100%;\">{title}</p>\n",
    "                                </a>\n",
    "                                <div class=\"d-flex align-items-center\">\n",
    "                                    <img class=\"img-fluid flex-shrink-0 rounded-circle\" src=\"{img}\" style=\"width: 50px; height: 50px;\">\n",
    "                                    <div class=\"ps-3\">\n",
    "                                        <h6 class=\"text-white mb-1\">\n",
    "                                            {company} \n",
    "                                            <a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"margin-left: 5px;\">\n",
    "                                                <i class=\"bi bi-box-arrow-up-right\"></i>\n",
    "                                            </a>\n",
    "                                        </h6>\n",
    "                                        <small>{date[:-3]}</small>\n",
    "                                    </div>\n",
    "                                </div>\n",
    "                            </div>\n",
    "        \"\"\"\n",
    "\n",
    "        # save the div\n",
    "        if date in divs:\n",
    "            divs[date].append(div)\n",
    "        else:\n",
    "            divs[date] = [div]\n",
    "\n",
    "    # sort the divs by date and prepare for output\n",
    "    divs = dict(sorted(divs.items(), key=lambda item: item[0], reverse=False))\n",
    "    divs = \"\\n\".join([div for divs_list in divs.values() for div in divs_list])\n",
    "\n",
    "    return divs, len(titles)\n",
    "\n",
    "\n",
    "\n",
    "# get the divs and the number of news\n",
    "divs, num_news = news_topicrelation(output[search_topic])\n",
    "\n",
    "# read and replace \"{{ atchim }}\" with divs\n",
    "#with open(\"0info_urls.html\", \"r\") as f:\n",
    "#    content = f.read()\n",
    "\n",
    "#content = content.replace(\"{{ atchim }}\", divs)\n",
    "\n",
    "# write the new content\n",
    "#with open(\"0.html\", \"w\") as f:\n",
    "#    f.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoMosaic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
