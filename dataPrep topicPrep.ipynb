{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2296983c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 17:08:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data03\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae60ec0",
   "metadata": {},
   "source": [
    "# Topic preprocessing\n",
    "\n",
    "This notebook contains the code to clear some useless topics and to merge some topics that are too similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08500fe3",
   "metadata": {},
   "source": [
    "read the text file about all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f53e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics:  1457234\n",
      "+-------------------------+\n",
      "|value                    |\n",
      "+-------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|\n",
      "|laboratório              |\n",
      "|advogado                 |\n",
      "|descrever                |\n",
      "|Soup Nazi                |\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read text folder\n",
    "df = spark.read.text(\"topics.txt\")\n",
    "\n",
    "print(\"Number of unique topics: \", df.select(\"value\").count())\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22caa7",
   "metadata": {},
   "source": [
    "split multiple topics into a list of topics (if they have more then 1 space, ex.: \"Saúde $\\ $ Covid-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34074ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------+\n",
      "|value                    |tokens                     |\n",
      "+-------------------------+---------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]              |\n",
      "|advogado                 |[advogado]                 |\n",
      "|descrever                |[descrever]                |\n",
      "|Soup Nazi                |[Soup Nazi]                |\n",
      "+-------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"tokens\",\n",
    "    F.split(F.col(\"value\"), \" {2,}\")\n",
    ")\n",
    "\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a7d34",
   "metadata": {},
   "source": [
    "get a lemma of existent topics \n",
    "\n",
    "- by checking the spelling of the word in Natura Dictionary (https://natura.di.uminho.pt/download/sources/Dictionaries/wordlists/)\n",
    "\n",
    "- by using the most common form of the word (ex.: \"Galp\", \"galp\" -> goes to most common form between them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615ed69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=18680Kb max_used=18687Kb free=112391Kb\n",
      " bounds [0x000000010a98c000, 0x000000010bbec000, 0x000000011298c000]\n",
      " total_blobs=7551 nmethods=6627 adapters=837\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    }
   ],
   "source": [
    "# read the dictionary\n",
    "dicitionary = spark.read.text(\"wordlist_utf8.txt\")\n",
    "\n",
    "# create map of lowercase words to their original form\n",
    "def get_word_map(df):\n",
    "    word_map = {}\n",
    "    for row in df.collect():\n",
    "        word = row.value.strip()\n",
    "        if len(word) > 0:\n",
    "            word_map[word.lower()] = word\n",
    "    return word_map\n",
    "word_map = get_word_map(dicitionary)\n",
    "word_map_broadcast = spark.sparkContext.broadcast(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b27e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all tokens\n",
    "tokens_df = df.select(F.explode(F.col(\"tokens\")).alias(\"token\"))\n",
    "\n",
    "# create map for most common form\n",
    "window_spec = Window.partitionBy(\"std_token\").orderBy(F.desc(\"count\"))\n",
    "tokens_df = tokens_df \\\n",
    "            .withColumn(\"std_token\", F.lower(F.col(\"token\"))) \\\n",
    "            .groupBy(\"std_token\", \"token\") \\\n",
    "            .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "            .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "            .filter(F.col(\"rank\") == 1) \\\n",
    "            .orderBy(\"std_token\") \\\n",
    "            .select(\"std_token\", \"token\", \"count\")\n",
    "std_to_token_dict = {\n",
    "    row['std_token']: row['token']\n",
    "    for row in tokens_df.collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7931b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------+\n",
      "|topic                    |token                      |\n",
      "+-------------------------+---------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]              |\n",
      "|advogado                 |[advogado]                 |\n",
      "|descrever                |[descrever]                |\n",
      "|Soup Nazi                |[Soup Nazi]                |\n",
      "+-------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# apply the maps to the tokens\n",
    "def map_token(word):\n",
    "    word = word.lower()\n",
    "    if word in word_map_broadcast.value:\n",
    "        return word_map_broadcast.value[word]\n",
    "    elif word in std_to_token_dict:\n",
    "        return std_to_token_dict[word]\n",
    "    else:\n",
    "        print(f\"Token not found in dictionary: {word}\")\n",
    "        return None\n",
    "\n",
    "df = df.rdd.flatMap(\n",
    "    lambda row: [[row[0], [map_token(token) for token in row[1]]]]) \\\n",
    "    .toDF([\"topic\", \"token\"])\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f98bb",
   "metadata": {},
   "source": [
    "get all possible combinations of topics within a topic (ex.: Banco de Portugal -> Banco, Portugal, Banco de Portugal)\n",
    "\n",
    "- avoid splitting names (Leonor Pereira -> Leonor, Pereira, Leonor Pereira)[*BAD*]\n",
    "\n",
    "- only split if any of the words is in the Natura Dictionary but not as a name (\"Banco de Portugal\" -> banco, Portugal, Banco de Portugal)[*OK*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de69911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 17:08:43 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 21 (TID 58): Attempting to kill Python Worker\n",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------------------------------------------+\n",
      "|topic                    |token                                                     |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2, ípsilon, ímpar, fugas, P3, P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]                                             |\n",
      "|advogado                 |[advogado]                                                |\n",
      "|descrever                |[descrever]                                               |\n",
      "|Soup Nazi                |[soup, nazi, Soup Nazi]                                   |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def is_first_letter_upper(s):\n",
    "    return s[0].isupper() if s else False\n",
    "    \n",
    "def squeeze_tokens(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "\n",
    "        # already a single token\n",
    "        ind_token = token.split(\" \")\n",
    "        if len(ind_token) == 1:\n",
    "            new_tokens.append(ind_token[0])\n",
    "            continue\n",
    "\n",
    "        # verify if the token is ok for split\n",
    "        valid_split = False\n",
    "        invalid_word = {\"uns\", \"por\", \"dos\", \"das\"}\n",
    "        for t in ind_token:\n",
    "            t = t.lower()\n",
    "            if len(t) >= 3 and t not in invalid_word and t in word_map_broadcast.value:\n",
    "                valid_split = not is_first_letter_upper(word_map_broadcast.value[t])\n",
    "                if valid_split:\n",
    "                    break\n",
    "        \n",
    "        # if valid split, split it\n",
    "        if valid_split:\n",
    "            for t in ind_token:\n",
    "                t_lower = t.lower()\n",
    "                if t_lower in word_map_broadcast.value:\n",
    "                    t = word_map_broadcast.value[t_lower]\n",
    "                elif t_lower in std_to_token_dict:\n",
    "                    t = std_to_token_dict[t_lower]\n",
    "                else:\n",
    "                    t = t\n",
    "                new_tokens.append(t)\n",
    "\n",
    "        new_tokens.append(token)\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "df = df.rdd.flatMap(\n",
    "    lambda row: [[row[0], squeeze_tokens(row[1])]]) \\\n",
    "    .toDF([\"topic\", \"token\"])\n",
    "                \n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6150",
   "metadata": {},
   "source": [
    "check for bad encoded characters and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3712208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------------------------------------------+\n",
      "|topic                    |token                                                     |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2, ípsilon, ímpar, fugas, P3, P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]                                             |\n",
      "|advogado                 |[advogado]                                                |\n",
      "|descrever                |[descrever]                                               |\n",
      "|Soup Nazi                |[soup, nazi, Soup Nazi]                                   |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# define valid Portuguese characters\n",
    "custom_valid = [\n",
    "    \"ã\", \"á\", \"à\", \"â\", \"Á\", \"Ã\",\n",
    "    \"é\", \"ê\", \"É\", \"Ê\",\n",
    "    \"í\", \"Í\",\n",
    "    \"ó\", \"ô\", \"õ\", \"Ó\", \"Õ\",\n",
    "    \"ú\", \"Ú\",\n",
    "    \"ç\", \"-\", \"º\", \"ª\"\n",
    "]\n",
    "\n",
    "# define a mapping for common encoding issues\n",
    "fix_map = {\n",
    "    \"ő\": \"õ\", \"Ő\": \"Õ\", \"ť\": \"ç\", \"ťo\": \"ção\",\n",
    "    \"Ã£\": \"ã\", \"Ã¡\": \"á\", \"Ãª\": \"ê\", \"Ã³\": \"ó\",\n",
    "    \"Ã­\": \"í\", \"Ã©\": \"é\", \"Ã§\": \"ç\", \"Ã‰\": \"É\",\n",
    "    \"Ãµ\": \"õ\", \"Ãº\": \"ú\", \"Ã‰\": \"É\", \"ů\": \"ó\",\n",
    "    \"ă\": \"ã\", \"ę\": \"ê\", \"¾\": \"ó\",\n",
    "}\n",
    "\n",
    "def valid_word_detection(text):\n",
    "    return not re.search(rf\"[^a-zA-Z0-9 {''.join(re.escape(c) for c in custom_valid)}]\", text or \"\")\n",
    "\n",
    "def fix_encoding_issues(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for enc in [\"latin1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return text.encode(enc).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "def manual_fix(text):\n",
    "    for wrong, right in fix_map.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "\n",
    "def fixed_words(word):\n",
    "    if valid_word_detection(word):\n",
    "        return word\n",
    "\n",
    "    else:\n",
    "        fixed_word = fix_encoding_issues(word)\n",
    "        fixed_word = manual_fix(fixed_word)\n",
    "\n",
    "        if valid_word_detection(fixed_word):\n",
    "            return fixed_word\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "df = df.rdd.flatMap(\n",
    "    lambda row: [(row[0], [fixed_words(token) for token in row[1] if fixed_words(token)])]) \\\n",
    "    .toDF([\"topic\", \"token\"])\n",
    "\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef066c1",
   "metadata": {},
   "source": [
    "remove duplicated tokens and invalid tokens such as \"2018\", \"Abr 2020\", \"\", \"de\", ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1486a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 17:08:52 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 25 (TID 62): Attempting to kill Python Worker\n",
      "25/04/16 17:08:52 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 25 (TID 62): Attempting to kill Python Worker\n",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------------------------------------------+\n",
      "|topic                    |token                                                     |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2, ípsilon, P3, ímpar, fugas, P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]                                             |\n",
      "|advogado                 |[advogado]                                                |\n",
      "|descrever                |[descrever]                                               |\n",
      "|Soup Nazi                |[Soup Nazi, nazi, soup]                                   |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 17:08:56 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 26 (TID 63): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def hours_detection(token):\n",
    "    return bool(re.search(r\"\\b(?:\\d{1,2}:\\d{2}|\\d{1,2}h\\d{2})\\b\", token))\n",
    "\n",
    "def date_detection(token):\n",
    "    months = bool(re.search(r\"\\b(?:Jan|Fev|Abr|Mai|Jun|Jul|Ago|Set|Out|Nov|Dez)\\b\", token))\n",
    "    march = bool(re.search(r\"\\b(?:\\d{1,2} Mar|Mar \\d{2,4})\\b\", token))\n",
    "    months2 = bool(re.search(r\"^(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    dateD = bool(re.search(r\"\\b(Janeiro|Fevereiro|Março|Abril|Maio|Junho|Julho|Agosto|Setembro|Outubro|Novembro|Dezembro)[ ]?\\d{2,4}\\b\", token, flags=re.IGNORECASE))\n",
    "    dateY = bool(re.search(r\"\\b\\d{1,2}(?:\\s+de)?\\s+(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    days = bool(re.search(r\"\\b(segunda-feira|terça-feira|quarta-feira|quinta-feira|sexta-feira|sábado|domingo)\\b\", token, flags=re.IGNORECASE))\n",
    "    hours = bool(re.search(r\"\\b\\d{1,2}:\\d{2}\\b\", token)) or bool(re.search(r\"\\b\\d{1,2}h\\d{2}\\b\", token))\n",
    "    minutes = bool(re.search(r\"\\b\\d{1,2}m\\d{2}\\b\", token))\n",
    "    return months or march or months2 or dateD or dateY or days or hours or minutes\n",
    "\n",
    "def invalid_tokens_detection(token):\n",
    "    if token.lower() in {\"de\", \"da\", \"do\", \"dos\", \"das\", \"e\", \"ou\", \"a\", \"o\", \"as\", \"os\",\n",
    "                         \"para\", \"com\", \"em\", \"na\", \"no\", \"por\", \"pelo\", \"pelos\", \"uma\",\n",
    "                         \"pelo\", \"pelas\", \"com\", \"sem\", \"sobre\", \"entre\", \"até\", \"um\",\n",
    "                         \"antes\", \"depois\", \"durante\", \"após\", \"segundo\", \"junto\"}:\n",
    "        return True\n",
    "    if len(token) < 2:\n",
    "        return True\n",
    "    if token.isdigit():\n",
    "        return True\n",
    "    if token == \"\":\n",
    "        return True\n",
    "    if hours_detection(token):\n",
    "        return True\n",
    "    if date_detection(token):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "df = df.rdd.flatMap(\n",
    "    lambda row: [(row[0], [token for token in row[1] if not invalid_tokens_detection(token)])])\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], list(set(row[1])))]) \\\n",
    "    .toDF([\"topic\", \"token\"])\n",
    "\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c43f1",
   "metadata": {},
   "source": [
    "remove stop words that *introduce* the topic, such as \"O Banco de Portugal\" -> \"Banco de Portugal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89eab31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 17:09:00 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 27 (TID 64): Attempting to kill Python Worker\n",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------------------------------------------+\n",
      "|topic                    |token                                                     |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2, ípsilon, P3, ímpar, fugas, P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]                                             |\n",
      "|advogado                 |[advogado]                                                |\n",
      "|descrever                |[descrever]                                               |\n",
      "|Soup Nazi                |[Soup Nazi, nazi, soup]                                   |\n",
      "+-------------------------+----------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/16 17:09:05 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 28 (TID 65): Attempting to kill Python Worker\n",
      "25/04/16 17:09:05 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 28 (TID 65): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remove_introductory_stopword(token):\n",
    "    lower_token = token.lower()\n",
    "\n",
    "    if lower_token.startswith((\"a \", \"o \")):\n",
    "        return token[2:]\n",
    "    if lower_token.startswith((\"as \", \"os \", \"um \", \"de \", \"da \", \"do \", \"em \")):\n",
    "        return token[3:]\n",
    "    if lower_token.startswith((\"uma \", \"uns \")):\n",
    "        return token[4:]\n",
    "    if lower_token.startswith(\"umas \"):\n",
    "        return token[5:]\n",
    "    \n",
    "    return token\n",
    "\n",
    "df = df.rdd.flatMap(\n",
    "    lambda row: [(row[0], [remove_introductory_stopword(token) for token in row[1]])]) \\\n",
    "    .toDF([\"topic\", \"token\"])\n",
    "\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42da302",
   "metadata": {},
   "source": [
    "save the topics mapping to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d84b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").json(\"topics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoMosaic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
