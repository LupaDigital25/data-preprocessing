{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2296983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data03\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae60ec0",
   "metadata": {},
   "source": [
    "# Topic preprocessing\n",
    "\n",
    "This notebook contains the code to clear some useless topics and to merge some topics that are too similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08500fe3",
   "metadata": {},
   "source": [
    "read the text file about all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f53e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics:  1457234\n",
      "+-------------------------+\n",
      "|value                    |\n",
      "+-------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|\n",
      "|laboratório              |\n",
      "|advogado                 |\n",
      "|descrever                |\n",
      "|Soup Nazi                |\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read text folder\n",
    "df = spark.read.text(\"topics.txt\")\n",
    "\n",
    "print(\"Number of unique topics: \", df.select(\"value\").count())\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22caa7",
   "metadata": {},
   "source": [
    "split multiple topics into a list of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34074ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------+\n",
      "|value                    |tokens                     |\n",
      "+-------------------------+---------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]              |\n",
      "|advogado                 |[advogado]                 |\n",
      "|descrever                |[descrever]                |\n",
      "|Soup Nazi                |[Soup Nazi]                |\n",
      "+-------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"tokens\",\n",
    "    F.split(F.col(\"value\"), \" {2,}\")\n",
    ")\n",
    "\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78682c1e",
   "metadata": {},
   "source": [
    "get all possible combinations of topics within a topic (ex.: Banco de Portugal -> Banco, Portugal, Banco de Portugal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3daf0ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 176:>                                                        (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------+\n",
      "|topic             |tokens                                     |\n",
      "+------------------+-------------------------------------------+\n",
      "|revisão           |[revisão, revisão]                         |\n",
      "|The New York Times|[The, New, York, Times, The New York Times]|\n",
      "|Thierry Breton    |[Thierry, Breton, Thierry Breton]          |\n",
      "|sério             |[sério, sério]                             |\n",
      "|Hospital Prisional|[Hospital, Prisional, Hospital Prisional]  |\n",
      "+------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.rdd.repartition(4)\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [token for tokens in row[1] for token in tokens.split()] + row[1])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268375a3",
   "metadata": {},
   "source": [
    "check for bad encoded characters and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1f0f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------------+\n",
      "|topic         |tokens                           |\n",
      "+--------------+---------------------------------+\n",
      "|Thierry Breton|[Thierry, Breton, Thierry Breton]|\n",
      "|arrebatar     |[arrebatar, arrebatar]           |\n",
      "|ferido        |[ferido, ferido]                 |\n",
      "|Efacec Abusos |[Efacec, Abusos, Efacec Abusos]  |\n",
      "|Vasco Franco  |[Vasco, Franco, Vasco Franco]    |\n",
      "+--------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# define valid Portuguese characters\n",
    "custom_valid = [\n",
    "    \"ã\", \"á\", \"à\", \"â\", \"Á\", \"Ã\",\n",
    "    \"é\", \"ê\", \"É\", \"Ê\",\n",
    "    \"í\", \"Í\",\n",
    "    \"ó\", \"ô\", \"õ\", \"Ó\", \"Õ\",\n",
    "    \"ú\", \"Ú\",\n",
    "    \"ç\", \"-\", \"º\", \"ª\"\n",
    "]\n",
    "\n",
    "# define a mapping for common encoding issues\n",
    "fix_map = {\n",
    "    \"ő\": \"õ\", \"Ő\": \"Õ\", \"ť\": \"ç\", \"ťo\": \"ção\",\n",
    "    \"Ã£\": \"ã\", \"Ã¡\": \"á\", \"Ãª\": \"ê\", \"Ã³\": \"ó\",\n",
    "    \"Ã­\": \"í\", \"Ã©\": \"é\", \"Ã§\": \"ç\", \"Ã‰\": \"É\",\n",
    "    \"Ãµ\": \"õ\", \"Ãº\": \"ú\", \"Ã‰\": \"É\", \"ů\": \"ó\",\n",
    "    \"ă\": \"ã\", \"ę\": \"ê\", \"¾\": \"ó\",\n",
    "}\n",
    "\n",
    "def valid_word_detection(text):\n",
    "    return not re.search(rf\"[^a-zA-Z0-9 {''.join(re.escape(c) for c in custom_valid)}]\", text or \"\")\n",
    "\n",
    "def fix_encoding_issues(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for enc in [\"latin1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return text.encode(enc).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "def manual_fix(text):\n",
    "    for wrong, right in fix_map.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "\n",
    "def fixed_words(word):\n",
    "    if valid_word_detection(word):\n",
    "        return word\n",
    "\n",
    "    else:\n",
    "        fixed_word = fix_encoding_issues(word)\n",
    "        fixed_word = manual_fix(fixed_word)\n",
    "\n",
    "        if valid_word_detection(fixed_word):\n",
    "            return fixed_word\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [fixed_words(token) for token in row[1] if fixed_words(token)])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b1dba",
   "metadata": {},
   "source": [
    "remove duplicated tokens and invalid tokens such as \"2018\", \"Abr 2020\", \"\", \"de\", ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b354121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------------------------------------+\n",
      "|topic                  |tokens                                             |\n",
      "+-----------------------+---------------------------------------------------+\n",
      "|África África          |[África África, África]                            |\n",
      "|Fuel Publicidade       |[Publicidade, Fuel, Fuel Publicidade]              |\n",
      "|SAPO Tek               |[Tek, SAPO Tek, SAPO]                              |\n",
      "|Museus                 |[Museus]                                           |\n",
      "|Amadora Recrudescimento|[Recrudescimento, Amadora Recrudescimento, Amadora]|\n",
      "+-----------------------+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def hours_detection(token):\n",
    "    return bool(re.search(r\"\\b(?:\\d{1,2}:\\d{2}|\\d{1,2}h\\d{2})\\b\", token))\n",
    "\n",
    "def date_detection(token):\n",
    "    months = bool(re.search(r\"\\b(?:Jan|Fev|Abr|Mai|Jun|Jul|Ago|Set|Out|Nov|Dez)\\b\", token))\n",
    "    march = bool(re.search(r\"\\b(?:\\d{1,2} Mar|Mar \\d{2,4})\\b\", token))\n",
    "    months2 = bool(re.search(r\"^(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    dateD = bool(re.search(r\"\\b(Janeiro|Fevereiro|Março|Abril|Maio|Junho|Julho|Agosto|Setembro|Outubro|Novembro|Dezembro)[ ]?\\d{2,4}\\b\", token, flags=re.IGNORECASE))\n",
    "    dateY = bool(re.search(r\"\\b\\d{1,2}(?:\\s+de)?\\s+(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    days = bool(re.search(r\"\\b(segunda-feira|terça-feira|quarta-feira|quinta-feira|sexta-feira|sábado|domingo)\\b\", token, flags=re.IGNORECASE))\n",
    "    return months or march or months2 or dateD or dateY or days\n",
    "\n",
    "def invalid_tokens_detection(token):\n",
    "    if token.lower() in {\"de\", \"da\", \"do\", \"dos\", \"das\", \"e\", \"ou\", \"a\", \"o\", \"as\", \"os\",\n",
    "                         \"para\", \"com\", \"em\", \"na\", \"no\", \"por\", \"pelo\", \"pelos\", \"uma\",\n",
    "                         \"pelo\", \"pelas\", \"com\", \"sem\", \"sobre\", \"entre\", \"até\", \"um\",\n",
    "                         \"antes\", \"depois\", \"durante\", \"após\", \"segundo\", \"junto\"}:\n",
    "        return True\n",
    "    if len(token) < 2:\n",
    "        return True\n",
    "    if token.isdigit():\n",
    "        return True\n",
    "    if token == \"\":\n",
    "        return True\n",
    "    if hours_detection(token):\n",
    "        return True\n",
    "    if date_detection(token):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [token for token in row[1] if not invalid_tokens_detection(token)])]\n",
    ")\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], list(set(row[1])))]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d08ca",
   "metadata": {},
   "source": [
    "remove stop words that *introduce* the topic, such as \"O Banco de Portugal\" -> \"Banco de Portugal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b1ef7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 191:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------------+\n",
      "|topic           |tokens                                |\n",
      "+----------------+--------------------------------------+\n",
      "|Thai Raksa Chart|[Chart, Thai Raksa Chart, Thai, Raksa]|\n",
      "|podia           |[podia]                               |\n",
      "|vitamina D      |[vitamina D, vitamina]                |\n",
      "|Portimão Mar    |[Mar, Portimão Mar, Portimão]         |\n",
      "|Somem           |[Somem]                               |\n",
      "+----------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 10:39:52 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 191 (TID 259): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remove_introductory_stopword(token):\n",
    "    lower_token = token.lower()\n",
    "\n",
    "    if lower_token.startswith((\"a \", \"o \")):\n",
    "        return token[2:]\n",
    "    if lower_token.startswith((\"as \", \"os \", \"um \", \"de \", \"da \", \"do \")):\n",
    "        return token[3:]\n",
    "    if lower_token.startswith((\"uma \", \"uns \")):\n",
    "        return token[4:]\n",
    "    if lower_token.startswith(\"umas \"):\n",
    "        return token[5:]\n",
    "    \n",
    "    return token\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [remove_introductory_stopword(token) for token in row[1]])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3f1a9",
   "metadata": {},
   "source": [
    "map tokens to some sort of lemma, ex.: \"Saúde\" = \"saúde\"\n",
    "\n",
    "the ideia is to have a lemma and all tokens mapped to it (variants of the lemma), then choose the most frequent variant as a lemma\n",
    "\n",
    "ex.: galp, Galp, GALP -> galp, but Galp is the most frequent variant, so it becomes the lemma: galp, Galp, GALP -> Galp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b82e988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|std_token  |token      |count|\n",
      "+-----------+-----------+-----+\n",
      "|tópicos    |Tópicos    |18581|\n",
      "|foto       |Foto       |15169|\n",
      "|portugal   |Portugal   |13868|\n",
      "|fotogaleria|Fotogaleria|13082|\n",
      "|mundo      |Mundo      |12329|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all tokens\n",
    "tokens_df = df.toDF([\"topic\", \"tokens\"]).select(F.explode(F.col(\"tokens\")).alias(\"token\"))\n",
    "\n",
    "\n",
    "# count the most frequenet variant of each token and select the most frequent one\n",
    "window_spec = Window.partitionBy(\"std_token\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "tokens_df = tokens_df \\\n",
    "            .withColumn(\"std_token\", F.lower(F.col(\"token\"))) \\\n",
    "            .groupBy(\"std_token\", \"token\") \\\n",
    "            .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "            .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "            .filter(F.col(\"rank\") == 1) \\\n",
    "            .select(\"std_token\", \"token\", \"count\") \\\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "\n",
    "tokens_df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb1211b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------------------+\n",
      "|topic         |tokens                           |\n",
      "+--------------+---------------------------------+\n",
      "|Sonae SR      |[Sr, Sonae, Sonae SR]            |\n",
      "|Contactado    |[Contactado]                     |\n",
      "|Unidos Podemos|[Podemos, Unidos, Unidos Podemos]|\n",
      "|Painel        |[Painel]                         |\n",
      "|instala       |[instala]                        |\n",
      "+--------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 10:41:02 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 224 (TID 317): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "std_to_token_dict = {\n",
    "    row['std_token']: row['token']\n",
    "    for row in tokens_df.collect()\n",
    "}\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [std_to_token_dict[token.lower()] for token in row[1]])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42da302",
   "metadata": {},
   "source": [
    "save the topics mapping to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d84b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.toDF([\"topic\", \"tokens\"]).write.mode(\"overwrite\").json(\"topics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoMosaic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
