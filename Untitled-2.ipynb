{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2296983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data03\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08500fe3",
   "metadata": {},
   "source": [
    "read the text file about all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f53e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique topics:  1457234\n",
      "+-------------------------+\n",
      "|value                    |\n",
      "+-------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|\n",
      "|laboratório              |\n",
      "|advogado                 |\n",
      "|descrever                |\n",
      "|Soup Nazi                |\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read text folder\n",
    "df = spark.read.text(\"topics.txt\")\n",
    "\n",
    "print(\"Number of unique topics: \", df.select(\"value\").count())\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22caa7",
   "metadata": {},
   "source": [
    "split multiple topics into a list of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34074ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------+\n",
      "|value                    |tokens                     |\n",
      "+-------------------------+---------------------------+\n",
      "|P2 Ípsilon Ímpar Fugas P3|[P2 Ípsilon Ímpar Fugas P3]|\n",
      "|laboratório              |[laboratório]              |\n",
      "|advogado                 |[advogado]                 |\n",
      "|descrever                |[descrever]                |\n",
      "|Soup Nazi                |[Soup Nazi]                |\n",
      "+-------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"tokens\",\n",
    "    F.split(F.col(\"value\"), \" {2,}\")\n",
    ")\n",
    "\n",
    "df.show(truncate=False, n=5)\n",
    "#df.coalesce(1).write.mode(\"overwrite\").json(\"topics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78682c1e",
   "metadata": {},
   "source": [
    "get all possible combinations of topics within a topic (ex.: Banco de Portugal -> Banco, Portugal, Banco de Portugal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3daf0ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------+\n",
      "|topic             |tokens                                     |\n",
      "+------------------+-------------------------------------------+\n",
      "|revisão           |[revisão, revisão]                         |\n",
      "|The New York Times|[The, New, York, Times, The New York Times]|\n",
      "|Thierry Breton    |[Thierry, Breton, Thierry Breton]          |\n",
      "|sério             |[sério, sério]                             |\n",
      "|Hospital Prisional|[Hospital, Prisional, Hospital Prisional]  |\n",
      "+------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 21:08:10 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 38 (TID 69): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.rdd.repartition(4)\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [token for tokens in row[1] for token in tokens.split()] + row[1])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268375a3",
   "metadata": {},
   "source": [
    "check for bad encoded characters and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f0f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------------+\n",
      "|topic                   |tokens                                               |\n",
      "+------------------------+-----------------------------------------------------+\n",
      "|sinceridade             |[sinceridade, sinceridade]                           |\n",
      "|comunidade internacional|[comunidade, internacional, comunidade internacional]|\n",
      "|África África           |[África, África, África África]                      |\n",
      "|linhagem                |[linhagem, linhagem]                                 |\n",
      "|Hanover                 |[Hanover, Hanover]                                   |\n",
      "+------------------------+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define valid Portuguese characters\n",
    "custom_valid = [\n",
    "    \"ã\", \"á\", \"à\", \"â\", \"Á\", \"Ã\",\n",
    "    \"é\", \"ê\", \"É\", \"Ê\",\n",
    "    \"í\", \"Í\",\n",
    "    \"ó\", \"ô\", \"õ\", \"Ó\", \"Õ\",\n",
    "    \"ú\", \"Ú\",\n",
    "    \"ç\", \"-\", \"º\", \"ª\"\n",
    "]\n",
    "\n",
    "# define a mapping for common encoding issues\n",
    "fix_map = {\n",
    "    \"ő\": \"õ\", \"Ő\": \"Õ\", \"ť\": \"ç\", \"ťo\": \"ção\",\n",
    "    \"Ã£\": \"ã\", \"Ã¡\": \"á\", \"Ãª\": \"ê\", \"Ã³\": \"ó\",\n",
    "    \"Ã­\": \"í\", \"Ã©\": \"é\", \"Ã§\": \"ç\", \"Ã‰\": \"É\",\n",
    "    \"Ãµ\": \"õ\", \"Ãº\": \"ú\", \"Ã‰\": \"É\", \"ů\": \"ó\",\n",
    "    \"ă\": \"ã\", \"ę\": \"ê\", \"¾\": \"ó\",\n",
    "}\n",
    "\n",
    "def valid_word_detection(text):\n",
    "    return not re.search(rf\"[^a-zA-Z0-9 {''.join(re.escape(c) for c in custom_valid)}]\", text or \"\")\n",
    "\n",
    "def fix_encoding_issues(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    for enc in [\"latin1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return text.encode(enc).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return text\n",
    "\n",
    "def manual_fix(text):\n",
    "    for wrong, right in fix_map.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "\n",
    "def fixed_words(word):\n",
    "    if valid_word_detection(word):\n",
    "        return word\n",
    "\n",
    "    else:\n",
    "        fixed_word = fix_encoding_issues(word)\n",
    "        fixed_word = manual_fix(fixed_word)\n",
    "\n",
    "        if valid_word_detection(fixed_word):\n",
    "            return fixed_word\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [fixed_words(token) for token in row[1] if fixed_words(token)])]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b1dba",
   "metadata": {},
   "source": [
    "remove duplicated tokens and invalid tokens such as \"2018\", \"Abr 2020\", \"\", \"de\", ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b354121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------------------------+\n",
      "|topic              |tokens                                  |\n",
      "+-------------------+----------------------------------------+\n",
      "|Nobel da Literatura|[Nobel da Literatura, Literatura, Nobel]|\n",
      "|façanha            |[façanha]                               |\n",
      "|Unidos Podemos     |[Podemos, Unidos, Unidos Podemos]       |\n",
      "|Higiene Urbana     |[Higiene Urbana, Higiene, Urbana]       |\n",
      "|PUB ZAPPING        |[PUB ZAPPING, ZAPPING, PUB]             |\n",
      "+-------------------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 21:08:15 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 46 (TID 73): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def hours_detection(token):\n",
    "    return bool(re.search(r\"\\b(?:\\d{1,2}:\\d{2}|\\d{1,2}h\\d{2})\\b\", token))\n",
    "\n",
    "def date_detection(token):\n",
    "    months = bool(re.search(r\"\\b(?:Jan|Fev|Abr|Mai|Jun|Jul|Ago|Set|Out|Nov|Dez)\\b\", token))\n",
    "    march = bool(re.search(r\"\\b(?:\\d{1,2} Mar|Mar \\d{2,4})\\b\", token))\n",
    "    months2 = bool(re.search(r\"^(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    dateD = bool(re.search(r\"\\b(Janeiro|Fevereiro|Março|Abril|Maio|Junho|Julho|Agosto|Setembro|Outubro|Novembro|Dezembro)[ ]?\\d{2,4}\\b\", token, flags=re.IGNORECASE))\n",
    "    dateY = bool(re.search(r\"\\b\\d{1,2}(?:\\s+de)?\\s+(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\\b\", token, flags=re.IGNORECASE))\n",
    "    days = bool(re.search(r\"\\b(segunda-feira|terça-feira|quarta-feira|quinta-feira|sexta-feira|sábado|domingo)\\b\", token, flags=re.IGNORECASE))\n",
    "    return months or march or months2 or dateD or dateY or days\n",
    "\n",
    "def invalid_tokens_detection(token):\n",
    "    if token.lower() in {\"de\", \"da\", \"do\", \"dos\", \"das\", \"e\", \"ou\", \"a\", \"o\", \"as\", \"os\",\n",
    "                         \"para\", \"com\", \"em\", \"na\", \"no\", \"por\", \"pelo\", \"pelos\", \"uma\",\n",
    "                         \"pelo\", \"pelas\", \"com\", \"sem\", \"sobre\", \"entre\", \"até\", \"um\",\n",
    "                         \"antes\", \"depois\", \"durante\", \"após\", \"segundo\", \"junto\"}:\n",
    "        return True\n",
    "    if len(token) < 2:\n",
    "        return True\n",
    "    if token.isdigit():\n",
    "        return True\n",
    "    if token == \"\":\n",
    "        return True\n",
    "    if hours_detection(token):\n",
    "        return True\n",
    "    if date_detection(token):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [token for token in row[1] if not invalid_tokens_detection(token)])]\n",
    ")\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], list(set(row[1])))]\n",
    ")\n",
    "\n",
    "df.toDF([\"topic\", \"tokens\"]).sample(fraction=0.1).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d08ca",
   "metadata": {},
   "source": [
    "remove stop words that *introduce* the topic, such as \"O Banco de Portugal\" -> \"Banco de Portugal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1ef7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 21:08:19 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 50 (TID 75): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|topic      |tokens       |\n",
      "+-----------+-------------+\n",
      "|calma      |[calma]      |\n",
      "|Direito    |[Direito]    |\n",
      "|Contactado |[Contactado] |\n",
      "|Codogno    |[Codogno]    |\n",
      "|Presidentes|[Presidentes]|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remove_introductory_stopword(token):\n",
    "    lower_token = token.lower()\n",
    "\n",
    "    if lower_token.startswith((\"a \", \"o \")):\n",
    "        return token[2:]\n",
    "    if lower_token.startswith((\"as \", \"os \", \"um \", \"de \", \"da \", \"do \")):\n",
    "        return token[3:]\n",
    "    if lower_token.startswith((\"uma \", \"uns \")):\n",
    "        return token[4:]\n",
    "    if lower_token.startswith(\"umas \"):\n",
    "        return token[5:]\n",
    "    \n",
    "    return token\n",
    "\n",
    "df = df.flatMap(\n",
    "    lambda row: [(row[0], [remove_introductory_stopword(token) for token in row[1]])]\n",
    ").toDF([\"topic\", \"tokens\"])\n",
    "\n",
    "df.sample(fraction=0.1).show(truncate=False, n=5)\n",
    "df.coalesce(4).write.mode(\"overwrite\").json(\"topics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3f1a9",
   "metadata": {},
   "source": [
    "map tokens to some sort of lemma, ex.: \"Saúde\" = \"saúde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73d040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics:  1457234\n",
      "+----------------------------------------------------------+-------------------------+\n",
      "|tokens                                                    |topic                    |\n",
      "+----------------------------------------------------------+-------------------------+\n",
      "|[Fugas, P2, P3, Ípsilon, Ímpar, P2 Ípsilon Ímpar Fugas P3]|P2 Ípsilon Ímpar Fugas P3|\n",
      "|[laboratório]                                             |laboratório              |\n",
      "|[advogado]                                                |advogado                 |\n",
      "|[descrever]                                               |descrever                |\n",
      "|[Soup Nazi, Nazi, Soup]                                   |Soup Nazi                |\n",
      "+----------------------------------------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"topics.json\")\n",
    "\n",
    "print(\"Number of topics: \", df.count())\n",
    "df.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dabf4a",
   "metadata": {},
   "source": [
    "set of all unique tokens\n",
    "\n",
    "if a.lower in set\n",
    "a = a.lower()\n",
    "\n",
    "if a.title in set\n",
    "a = a.title()\n",
    "\n",
    "if a.upper in set\n",
    "a = a.upper()\n",
    "\n",
    "else\n",
    "\n",
    "a\n",
    "\n",
    "!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|token                            |\n",
      "+---------------------------------+\n",
      "|- A                              |\n",
      "|- APREN                          |\n",
      "|- Agricultura e Pescas           |\n",
      "|- Amnistia UE                    |\n",
      "|- As crianças precisam de atenção|\n",
      "|- Camilo Lourenço                |\n",
      "|- Clima - Correio da Manhã       |\n",
      "|- Conjuntura                     |\n",
      "|- Conjuntura -                   |\n",
      "|- Convidado                      |\n",
      "|- Convidado 1                    |\n",
      "|- Convidado 11                   |\n",
      "|- Convidado 15                   |\n",
      "|- Convidado 17                   |\n",
      "|- Convidado 2                    |\n",
      "|- Convidado 28                   |\n",
      "|- Convidado 9                    |\n",
      "|- Convidado Hoje                 |\n",
      "|- Convidado Hoje Do Syriza       |\n",
      "|- Convidado Hoje Jogar           |\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get all unique tokens\n",
    "tokens_df = df.select(F.explode(F.col(\"tokens\")).alias(\"token\"))\n",
    "unique_tokens = tokens_df.select(\"token\").distinct().orderBy(\"token\")\n",
    "\n",
    "# Show the sorted unique tokens\n",
    "unique_tokens.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3111609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfoMosaic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
